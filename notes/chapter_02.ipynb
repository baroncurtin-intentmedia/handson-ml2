{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Chapter 2 - End-to-End Machine Learning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1 Working with Real Data\n",
    "\n",
    "- Various places to find real-world data:\n",
    "    - Open Data Repositories\n",
    "        - [UC Irvine Machine Learning Repository](http://archive.ics.uci.edu/ml)\n",
    "        - [Kaggle Datasets](https://www.kaggle.com/datasets)\n",
    "        - [Amazon's AWS Datasets](https://registry.opendata.aws)\n",
    "    - Meta Portals\n",
    "        - [Data Portals](http://www.dataportals.org)\n",
    "        - [OpenDataMonitor](http://opendatamonitor.eu)\n",
    "        - [Quandl](http://quandl.com)\n",
    "    - Other Pages\n",
    "        - [Wikipedia's list of ML Datasets](https://homl.info/9)\n",
    "        - [Quora.com](https://homl.info/10)\n",
    "        - [Reddit Datasets Subreddit](https://www.reddit.com/r/datasets)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 Look at the Big Picture\n",
    "\n",
    "- The task in this chapter is to use California census data to build a model of housing prices in the state\n",
    "- The model should learn from the data and be able to predict the median housing price in any district, given the other metrics\n",
    "\n",
    "### 2.2.1 Frame the Problem\n",
    "\n",
    "- Frame the Problem/Look at Big Picture Checklist:\n",
    "    - Define the objective in business terms\n",
    "    - How will your solution be used?\n",
    "    - What are the current solutions/workarounds (if any)?\n",
    "    - How should you frame this problem (supervised/unsupervised, online/offline, etc.?)\n",
    "    - How should performance be measured?\n",
    "    - Is the performance measure aligned with the business objective?\n",
    "    - What would be the minimum performance needed to reach the business objective?\n",
    "    - What are comparable problems? Can you reuse experience or tools?\n",
    "    - Is human expertise available?\n",
    "    - How would you solve the problem manually?\n",
    "    - List the assumptions you (or others) have made so far.\n",
    "    - Verify the assumptions if possible?\n",
    "- Data Pipelines\n",
    "    - A sequence of data processing components\n",
    "    - Components typically run asynchronously\n",
    "        - Each component pulls in a large amount of data, processes it, and spits out the result in another data store\n",
    "        - If a component breaks down, the downstream components can often continue to run normally (at east for a while)\n",
    "        by just using the last output from the broken component\n",
    "        - A broken component can go unnoticed for some time if proper monitoring is not implemented\n",
    "\n",
    "### 2.2.2 Select a Performance Measure\n",
    "\n",
    "- A typical performance measure for regression problems is the Root Mean Square Error (RMSE)\n",
    "$$RMSE(\\textbf{X},h) = \\sqrt{\\frac{1}{m} \\sum_{i=1}^{m} (h(\\textbf{x}^{(i)}) - y^{(i)})^2 }$$\n",
    "    - $m$ is the number of instances in the dataset you are measuring the RMSE on\n",
    "    - $\\textbf{x}^{(i)}$ is a vector of all the feature values (excluding the label) of the $i^{th}$ instance in the dataset, and $y^{(i)}$ \n",
    "    is its label (the desired output value for that instance)\n",
    "    - $\\textbf{X}$ is a matrix containing all the feature values (excluding labels) of all instances in the dataset.\n",
    "    There is one row per instance, and the $i^{th}$ row is equal to the transpose of $\\textbf{x}^{(i)}$, noted $(\\textbf{x}^{(i)})^\\intercal$\n",
    "    - $h$ is your system's prediction function, also called a *hypothesis*. When your system is given an instance's feature\n",
    "    vector $\\textbf{x}^{(i)}$, it outputs a predicted value $\\hat{y}^{(i)} = h(\\textbf{x}^{(i)})$\n",
    "- Mean Absolute Error (MAE) or Average Absolute Deviation\n",
    "    - Good for when data contains a lot of outliers\n",
    "$$MAE(\\textbf{X},h) = \\frac{1}{m} \\sum_{i=1}^{m} \\left| h(\\textbf{x}^{(i)}) - y^{(i)}\\right|$$\n",
    "\n",
    "- Both RMSE and MAE are ways to measure distance between two vectors\n",
    "    - Computing the RMSE corresponds to the *Euclidean norm*\n",
    "        - Also called the $l_2$ norm, noted $||x||_2$\n",
    "    - Computing the Mean Absolute Error (MAE) corresponds to the *Manhattan Norm*\n",
    "        - Also called the $l_1$ norm, noted $||x||_1$\n",
    "        - Measures the distance between two points in a city if you can only travel along orthogonal city blocks (right angles)\n",
    "    - Generally speaking, the $l_k$ norm vector $\\textbf{v}$ containing *n* elements is defined as  $||\\textbf{v}||_k$\n",
    "        - $l_0$ gives the number of nonzero elements in the vector\n",
    "        - $l_{\\inf}$ gives the maximum absolute value in the vector\n",
    "    - The higher the norm index, the more if focuses on large values and neglects small ones\n",
    "        - RMSE is more sensitive to outliers because of this but when the distribution is normal, RMSE performs very well\n",
    "        and is the preferred performance measure\n",
    "\n",
    "### 2.2.3 Check the Assumptions\n",
    "\n",
    "- It is good practice to list and review the assumptions to catch serious issues early on"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3 Get the Data\n",
    "\n",
    "- Get the Data Checklist:\n",
    "    - List the data you need and how much you need\n",
    "    - Find and document where you can get that data\n",
    "    - Check how much space it will take\n",
    "    - Check legal obligations, and get authorization if necessary\n",
    "    - Get access authorizations\n",
    "    - Create a workspace (with enough storage space)\n",
    "    - Get the data\n",
    "    - Convert the data to a format you can easily manipulate (without changing the data itself)\n",
    "    - Ensure sensitive information is deleted or protected (e.g. anonymized)\n",
    "    - Check the size and type of data (time series, sample, geographical, etc.)\n",
    "    - Sample a test set, put it aside, and never look at it (no data snooping)\n",
    "    \n",
    "### 2.3.1 Create the Workspace\n",
    "\n",
    "- Workspace should be a virtualenv or conda env with the following packages:\n",
    "    - Jupyter\n",
    "    - NumPy\n",
    "    - pandas,\n",
    "    - Matplotlib\n",
    "    - Scikit-Learn\n",
    "\n",
    "### 2.3.2 Download the Data\n",
    "\n",
    "- In typical environments, your data would be available in a relational database (or some other data store) and spread\n",
    "across multiple tables/documents/files\n",
    "\n",
    "### 2.3.4 Take a Quick Look at the Data Structure\n",
    "\n",
    "- The `.head()` method in pandas allows you to look at the top five rows of the data\n",
    "- The `.info()` method in pandas is useful to get a quick description of the data\n",
    "    - Provides total number of rows\n",
    "    - Each attribute's type\n",
    "    - Number of nonnull values\n",
    "        - Missing values need to be taken care of in the data cleaning step\n",
    "- For categorical columns, it is useful to use the `.value_counts()` method of a pandas series\n",
    "- The `.describe()` method shows a summary of the numerical attributes\n",
    "- The `.hist()` method plots a histogram of all numerical attributes in the dataset\n",
    "    - Important to note that skewed distributions need to be manipulated to be more bell-shaped\n",
    "\n",
    "### 2.3.5 Create a Test Set\n",
    "\n",
    "- A *test set* is created by randomly sampling 20% of the dataset and setting it aside\n",
    "    - When the dataset is large enough, random sampling methods work well enough but if it is not, there is a great risk\n",
    "    of significant sampling bias\n",
    "    - *Stratified sampling* divides the population into homogeneous subgroups called *strata* and the correct number of \n",
    "    instances are sampled from each stratum to guarantee that the test set is representative of the overall population\n",
    "        - Sklearn provides the `StratifiedSuffleSplit` class to do stratified sampling\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.4 Discover and Visualize the Data to Gain Insights\n",
    "\n",
    "- After taking a quick glance at the data and creating a test set, the next step is to gain greater understanding through\n",
    "visualization\n",
    "- Using the `.copy()` method of pandas Dataframes is helping in preserving the integrity of the training set\n",
    "\n",
    "### 2.4.1 Visualizing Geographical Data\n",
    "\n",
    "- Dealing with the housing dataset, we have to visualize geographical data and this is best done through use of a scatterplot\n",
    "    - Setting the parameter `alpha=.1` will make it easier to see high density data points\n",
    "    - To further aid the analysis, we can augment the radius of the circle to represent the population, the color to the\n",
    "    price\n",
    "\n",
    "### 2.4.2 Looking for Correlations\n",
    "\n",
    "- In datasets that aren't large, the *standard correlation coefficient* (Pearson's r) can be calculated between every pair\n",
    "of attributes using the `.corr()` method\n",
    "    - The value ranges from -1 to 1\n",
    "    - Correlations close to 1 signify strong positive correlations\n",
    "    - Correlations close to -1 signify strong negative correlations\n",
    "    - Correlations close to 0 signify very weak or no correlation at all\n",
    "    - Important to note that the correlation has nothing to do with the slope of the relationship\n",
    "- Another way of getting the correlations between attributes is to use the `.scatter_matrix()` function in pandas\n",
    "    - Note: produces a figure of $n^2$ plots, $n$ being the number of numerical attributes in the dataset\n",
    "\n",
    "### 2.4.3 Experimenting with Attribute Combinations\n",
    "\n",
    "- Creating new attribute combinations is the process of using existing features, and calculating new ones through arithmetic\n",
    "or some other means\n",
    "- Often times, new attributes or combinations of attributes will be more correlated with the target value\n",
    "- The first round of exploration does not have to be thorough\n",
    "    - The idea is to get a prototype up and running and then iterate on the creative process"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.5 Prepare the Data for Machine Learning Algorithms\n",
    "\n",
    "- After the exploration phase, it's time to prepare the data for the algorithm\n",
    "    - It is best practice to create a series of functions for the purposes of this step because:\n",
    "        - Reproducibility\n",
    "        - Building a library of functions for re-usability\n",
    "        - Can be transferred over to production\n",
    "\n",
    "### 2.5.1 Data Cleaning\n",
    "\n",
    "- Most ML algorithms cannot work with missing features\n",
    "- There are a few options when dealing with missing features\n",
    "    - Getting rid of the row of data\n",
    "    - Getting rid of the column (attribute)\n",
    "    - Imputing the values (zero, mean, median, etc)\n",
    "        - When using this option, it is important to calculate only for the training set and carry that value over to the\n",
    "        test set\n",
    "        - Sklearn has a `SimpleImputer` class that can be used for simple imputation but can only be used on numerical attributes\n",
    "        \n",
    "- Sci-Kit Learn Design\n",
    "    - All objects share a consistent and simple interface\n",
    "    - *Estimators*\n",
    "        - Any object that can estimate some parameters based on a dataset is called an *estimator*\n",
    "        - Estimation is performed by the `.fit()` method\n",
    "        - Will generally only take the dataset as a parameter, along with the labels in a supervised learning algorithm\n",
    "        - All other parameters are hyper-parameters and must be set as an instance variable\n",
    "    - *Transformers*\n",
    "        - Estimators that can also transform the dataset (imputer is an example)\n",
    "        - Transformations are performed via the `.transform()` method\n",
    "        - All transformers also have a `.fit_transforms()` method that does both `.fit()` and `.transform()` and is often\n",
    "        more optimized than doing the two operations separately\n",
    "    - *Predictors*\n",
    "        - Estimators that are capable of making predictions are called *predictors*\n",
    "    - *Inspection*\n",
    "        - All the estimator's hyper-parameters are accessible directly via public instance variables\n",
    "        - All the estimator's learned parameters are accessible via public instance variables with an underscore suffix\n",
    "\n",
    "### 2.5.2 Handling Text and Categorical Attributes\n",
    "\n",
    "- For ordinal columns, categorical columns that have a sense of order to them, Sklearn provides the `OrdinalEncoder` class\n",
    "- For categorical columns that are not ordinal, Sklearn provides the `OneHotEncoder` class\n",
    "    - The result of the `OneHotEncoder` class is a sparse matrix to preserve memory\n",
    "- If a categorical attribute has a large number of possible categories, then one-hot encoding will result in a large number\n",
    "of input features\n",
    "    - This can be handled by:\n",
    "        - Replacing the categorical input with useful numerical features\n",
    "        - Replace each category with a learnable, low-dimensional vector called an *embedding*\n",
    "            - This is called *representation learning*\n",
    "\n",
    "### 2.5.3 Custom Transformers\n",
    "\n",
    "- Sometimes you will need to write your own transformers and they will need to implement the `.fit()`, `.transform()`, and\n",
    "the `.fit_transform()` methods so that it seamlessly integrates with the rest of the Sklearn API\n",
    "- Many data preparation steps can be automated with Sklearn's base transformers and a custom transformer allowing the data\n",
    "scientist to find many combinations in minimal time\n",
    "\n",
    "### 2.5.4 Feature Scaling\n",
    "\n",
    "- *Feature scaling* is one of the most important transformations for ML\n",
    "    - ML algorithms typically do not perform well when numerical attributes have different scales\n",
    "    - Scaling is generally not required on target values\n",
    "- There are two common ways to get all attributes to have the same scale:\n",
    "    - *Min-max scaling* (Normalization): values are shifted and rescaled so that they end up ranging from 0 to 1\n",
    "        - $X_{normalized} = \\frac{X - X_{min}}{X_{max}-X_{min}}$\n",
    "        - Sklearn provides a `MinMaxScaler` transformer\n",
    "        - Heavily influenced by outliers\n",
    "    - *Standardization*: values are shifted so that the resulting distribution has a unit variance\n",
    "        - $X_{standardized} = \\frac{X - X_{mean}}{X_{std}}$\n",
    "        - Sklearn provides a `StandardScaler` transformer for this\n",
    "        - Standardization is not affected by outliers\n",
    "- Important to note that the transformers of `MinMaxScaler` and `StandardScaler` should only be fit to the training set\n",
    "\n",
    "### 2.5.5 Transformation Pipelines\n",
    "\n",
    "- Sklearn provides the `Pipeline` class to make sequences of transformations easily repeatable\n",
    "    - The `Pipeline` constructor takes a list of name/estimator pairs\n",
    "    - All but the last estimator must be transformers (they must have the `.fit_transform()` method\n",
    "- In v.20, Sklearn provides the `ColumnTransformer` class that allows users to specify different transformations for a set\n",
    "of columns\n",
    "    - A set of transformations can be defined for numerical columns\n",
    "    - A set of transformations can be defined for categorical columns\n",
    "    - When `.fit()` is used, the different transformations are applied appropriately for the defined columns "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.6 Select and Train a Model\n",
    "\n",
    "- Follows framing the problem, getting the data, exploration, splitting into training and test sets, transforming the data,\n",
    "cleaning up the data, and preparing the data for the ML algorithms0\n",
    "\n",
    "### 2.6.1 Training and Evaluating on the Training Set\n",
    "\n",
    "- *Underfitting* is when the features do not provide enough predictive information\n",
    "    - Solutions are to use a more powerful model\n",
    "    - Create better features\n",
    "    - Reduce the constraints on the model   \n",
    "\n",
    "### 2.6.2 Better Evaluation Using Cross-Validation\n",
    "\n",
    "- One way to evaluate models (and the better way) is to split the training set into a smaller training set and a validation set\n",
    "    - Train the models on the smaller training set\n",
    "    - Evaluate against the validation set\n",
    "- Sklearn provides a *K-fold cross-validation feature* that randomly splits the training set into $k$ subsets called *folds*,\n",
    "and then trains and evaluates the model $k$ times, picking a different fold for evaluation every time and training on the\n",
    "other $k-1$ folds\n",
    "    - The result is an array with $k$ evaluation scores\n",
    "- Cross-validation allows you to get an estimate of the performance, but also how precise the estimate is\n",
    "- Building a model on top of many other models is called *ensemble learning*\n",
    "    - An example is a Random Forest\n",
    "- A sign of over-fitting is when the training set's evaluation score is much lower than the validation sets\n",
    "    - Possible solutions are:\n",
    "        - Constrain (regularize it)\n",
    "        - Get more data\n",
    "- Models you experiment with should be saved so that you can come back to them easily\n",
    "    - Both the hyper-parameters and the trained parameters should be saved\n",
    "    - Cross-validation scores should be saved\n",
    "    - Predictions could also be saved\n",
    "    - Sklearn models can easily be saved via Python's `pickle` model or using the `joblib` library"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.7 Fine-Tune Your Model\n",
    "\n",
    "- Once you have a shortlist of promising models, the models then need to be fine-tuned\n",
    "\n",
    "### 2.7.1 Grid Search\n",
    "\n",
    "- The first option to fine-tuning a model is to experiment with hyper-parameters\n",
    "- Sklearn provides the `GridSearchCV` class to perform the search for you\n",
    "    - Tell Sklearn what hyper-parameters you want it to experiment with\n",
    "    - It will use cross-validation to evaluate all the possible combinations of hyper-parameter values\n",
    "    - When you don't know what value a hyper-parameter should have, a simple approach is to try out consecutive powers of 10\n",
    "\n",
    "### 2.7.2 Randomized Search\n",
    "\n",
    "- Randomized search is the alternative to Grid Search and is preferred when the hyper-parameter space is large\n",
    "    - Sklearn provides the `RandomizedSearchCV` class\n",
    "    - This will evaluate a given number of random combinations by selecting a random value for each hyperparameter at every\n",
    "    iteration\n",
    "    - The two main benefits of Randomized Search are:\n",
    "        - It will let you evaluate $x$ different values for every hyper-parameter as opposed to a few using Grid Search\n",
    "        - The number of iterations can be set so the computing budget can be controlled\n",
    "\n",
    "### 2.7.3 Ensemble Methods\n",
    "\n",
    "- Another way to fine-tune the system is to combine the models\n",
    "- The group (*ensemble*) of the \"best\" models will often perform better than the best individual models, especially if \n",
    "the individual models make very different types of errors\n",
    "\n",
    "### 2.7.4 Analyze the Best Models and Their Errors\n",
    "\n",
    "- You will often gain good insights on the problem by inspecting the best models\n",
    "- With this information, dropping some of the less useful features becomes accessible\n",
    "- Looking at the specific errors the system makes and why the model makes them could lead to solutions to fix the problem\n",
    "    - This could be solved via:\n",
    "        - Adding extra features\n",
    "        - Getting rid of uninformative ones\n",
    "        - Cleaning up outliers\n",
    "\n",
    "### 2.7.5 Evaluate Your System on the Test Set\n",
    "\n",
    "- After fine-tuning the models, its time to evaluate on the *test set*\n",
    "- To do so, you call `.transform()` using the pipeline you created prior\n",
    "- You might want to have an idea of how precise this estimate is and you can use the `scipy.stats.t.interval()` function\n",
    "to compute a *95% confidence interval* for the generalization error\n",
    "- A lot of hyper-parameter tuning will usually lead to a *test set* evaluation that performs slightly worse than cross-validation\n",
    "- Prior to launching, you need to:\n",
    "    - Present your solution (highlighting lessons learned)\n",
    "    - What worked and what did not\n",
    "    - What assumptions were made\n",
    "    - What the system's limitations are "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.8 Launch, Monitor, and Maintain Your System\n",
    "\n",
    "- After getting approval to launch the model into production, it needs to be deploy\n",
    "    - One way to do this is to save the trained Sklearn model (e.g. using `joblib`), including the full preprocessing and\n",
    "    prediction pipeline, then load this trained model within your production environment and use it to make predictions by\n",
    "    calling the `.predict()` method\n",
    "- Once the model is deploy, code needs to be written to monitor the system's live performance at regular intervals and\n",
    "trigger alerts when it drops\n",
    "- In some cases, the model's performance can be inferred from downstream metrics\n",
    "- A monitoring system needs to be put in place as well as all the relevant processes to define what to do in case of failures\n",
    "and how to prepare for them"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}